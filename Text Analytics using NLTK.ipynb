{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEXT ANALYTICS**<br/>\n",
    "To illustrate the basic concepts behind the analysis of text, a short collection of magazine stories will be use called \"Astounding Stories\" available from http://www.gutenberg.org. These are contained in a single file. The file will be read into a single string. The string will be parsed and filter to prepare the term-document matrix. Each row of this matrix will represent a single term, a word, number, date or\n",
    "entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import operator\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the lists files and term_doc are initialized. fiiles contains the name of the text documents that will be read and used to create the term-document matrix. These are books available from www.gutenberg.org. term_doc is a matrix that will be populated with the terms and their count for each document when run in a loop.\n",
    "\n",
    "The last three binary variables , pos_tag, stemming, stop words are set to True or False to describe which of the four scenarios are being processed\n",
    "1. POS(Yes) Stop(Yes) Stem(Yes)\n",
    "2. POS(Yes) Stop(No) Stem(No)\n",
    "3. POS(Yes) Stop(No) Stem(No)\n",
    "4. POS(No) Stop(No) Stem(No)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/gaura/Downloads/TAMU/656/Week 9 HW/TextFiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt', 'T7.txt', 'T8.txt']\n",
    "term_doc = []\n",
    "pos_tags = True\n",
    "stemming = True\n",
    "remove_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Each Document**<br/>\n",
    "The main part of this code loops over individual documents. Each is processed using the following steps:\n",
    "1. Read\n",
    "2. Preprocessing anf Tokenization\n",
    "3. Parts of Speech\n",
    "4. Stop Words\n",
    "5. Stem\n",
    "6. Add to term_doc list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document T1.txt contains a total of 86484  terms.\n",
      "Document T1.txt contains 40039 terms after stemming.\n",
      "\n",
      "\n",
      "Document T2.txt contains a total of 108474  terms.\n",
      "Document T2.txt contains 48289 terms after stemming.\n",
      "\n",
      "\n",
      "Document T3.txt contains a total of 104778  terms.\n",
      "Document T3.txt contains 50177 terms after stemming.\n",
      "\n",
      "\n",
      "Document T4.txt contains a total of 83140  terms.\n",
      "Document T4.txt contains 35269 terms after stemming.\n",
      "\n",
      "\n",
      "Document T5.txt contains a total of 76238  terms.\n",
      "Document T5.txt contains 35030 terms after stemming.\n",
      "\n",
      "\n",
      "Document T6.txt contains a total of 35136  terms.\n",
      "Document T6.txt contains 15670 terms after stemming.\n",
      "\n",
      "\n",
      "Document T7.txt contains a total of 80206  terms.\n",
      "Document T7.txt contains 35461 terms after stemming.\n",
      "\n",
      "\n",
      "Document T8.txt contains a total of 64511  terms.\n",
      "Document T8.txt contains 29797 terms after stemming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "\n",
    "\n",
    "# Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "# Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "# Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    adoc = adoc.replace(\"'d\", \" \")\n",
    "# Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and \\\n",
    "    word != \"''\" and word !=\"``\"]\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "\n",
    "    if pos_tags:\n",
    "# POS Tagging\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "    if remove_stop:\n",
    "# Remove stop words\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "# Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "# Remove stop words\n",
    "        if pos_tags:\n",
    "            tokens = [word for word in tokens if word[0] not in stop]\n",
    "            tokens = [word for word in tokens if (not word[0].replace('.','',1).isnumeric()) and word[0]!=\"'s\" ]\n",
    "        else:\n",
    "            tokens = [word for word in tokens if word not in stop]\n",
    "            tokens = [word for word in tokens if word != \"'s\" ]\n",
    "\n",
    "    if stemming:\n",
    "# WordNet Lematization Stems using POS\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        if pos_tags:\n",
    "            for token in tokens:\n",
    "                term = token[0]\n",
    "                pos = token[1]\n",
    "                pos = pos[0]\n",
    "                try:\n",
    "                    pos = wn_tags[pos]\n",
    "                    stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "                except:\n",
    "                    stemmed_tokens.append(stemmer.stem(term))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                stemmed_tokens.append(stemmer.stem(token))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \"terms after stemming.\\n\")\n",
    "        tokens = stemmed_tokens\n",
    "    fdist = FreqDist(tokens)\n",
    "# Use with Wordnet\n",
    "    td= {}\n",
    "#term_doc = []\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**Term-Document Matrix** <br/>\n",
    "This ends the loop over individual documents. At this point point, the matrix term_doc contains the counts for each term and each document, but the terms in multiple documents need to be combined. We create a dictionary *td_matrix* which combines the terms of different documents together in one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**Printing Term-Document Matrix** <br/>\n",
    "The term Document matrix is first sorted in descending order of frequency of each term and then printed along with the scenario description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: POS= True Remove Stop Words= True  Stemming= True\n",
      "------------------------------------------------------------\n",
      "TERM            TOTAL  D1  D2   D3   D4   D5   D6   D7   D8\n",
      "one             2127  291  437  348  211  312  121  202  205\n",
      "water           2040   47  922  825    7   94    7   55   83\n",
      "make            1928  204  694  262  185  237   63  169  114\n",
      "would           1855  270  407  195  309  222   60  289  103\n",
      "go              1620  212  292   18  239  154  103  374  228\n",
      "come            1511  211  153   62  126  276  155  282  246\n",
      "could           1363  221  121   49  364  195   93  203  117\n",
      "time            1333  137  128  175  167  164  213  216  133\n",
      "see             1188  179  232  129  156  110   72  172  138\n",
      "light           1175   87  461  322   21   92   61   60   71\n",
      "get             1146  171  291   24   76  121   53  315   95\n",
      "air             1126   69  518  412   20   19   23   30   35\n",
      "know            1042  165  102  112  223  119   46  202   73\n",
      "day              939   87   52   82  117  337   49  107  108\n",
      "take             926  129  174   82  110  135   52  178   66\n",
      "upon             891  168   11  163   88   28  113  148  172\n",
      "way              844   78  211  122   73  100   42  116  102\n",
      "thing            832  120  241   10   76   57  100  113  115\n",
      "like             828  173  119   56  100   80   74  130   96\n",
      "man              827  248   38  104   90   90   70   62  125\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"TERM            TOTAL  D1  D2   D3   D4   D5   D6   D7   D8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    \n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
